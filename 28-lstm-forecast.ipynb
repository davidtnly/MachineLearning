{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Predicting Sales\n",
    "Notes from this notebook comes from various resources listed at the end of the notebook. This will be good exposure to using more plotly to produce interactable plots vs. matplotlib as well predictive modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Areas of Focus\n",
    "\n",
    "- Forecasting time series\n",
    "- Long Short-term Memory (LSTM)\n",
    "- Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Problem\n",
    "\n",
    "We want to know how much sales we will have in the future. This can help the business in multiple ways. It can be used as a benchmark. We can also calculate the incremental value of new actions on top of this benchmark. Are our strategies going to break above this benchmark or is it going to underperform?\n",
    "\n",
    "Second, we can use it for planning. Many demand and supply teams want to figure out what's the most efficieny number to order and have. We don't want too many and it won't sell out completely or do we want to miscalculate and not order enough for our supplies and lose out on potential sales. By planning ahead, we can look at the forecasts and see where we can invest more.\n",
    "\n",
    "Complimentary to planning for demand and supply, we could also plan for budgets and targets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process\n",
    "\n",
    "- EDA\n",
    "- Data Transformation to make it stationary and supervised\n",
    "    - Ways to deal with seasonality?\n",
    "    - Should stores be modeled separately or pooled together?\n",
    "    - Does deep learning work better than ARIMA? XGBoost?\n",
    "- Building the LSTM model & evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "5 years of store-item sales data, and asked to predict 3 months of sales for 50 different items at 10 different stores.\n",
    "\n",
    "- date - Date of the sale data. There are no holiday effects or store closures.\n",
    "- store - Store ID\n",
    "- item - Item ID\n",
    "- sales - Number of items sold at a particular store on a particular date.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotly'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-c449bcb2c0f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# import plotly.plotly as py\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph_objs\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mgo\u001b[0m \u001b[1;31m# graph_objects in version 4 (currently have plotly v3 installed)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moffline\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpyoff\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'plotly'"
     ]
    }
   ],
   "source": [
    "# Toolbox 101\n",
    "from datetime import datetime, timedelta,date\n",
    "import pandas as pd\n",
    "import pandas_profiling\n",
    "import numpy as np\n",
    "import random as rand\n",
    "import datetime as dt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# import plotly.plotly as py\n",
    "import plotly.graph_objs as go # graph_objects in version 4 (currently have plotly v3 installed)\n",
    "import plotly.offline as pyoff\n",
    "\n",
    "# Stats\n",
    "from scipy.stats import norm, skew\n",
    "from scipy.special import boxcox1p\n",
    "from scipy.stats import boxcox_normmax\n",
    "from scipy import stats\n",
    "\n",
    "# Deep learning\n",
    "# import keras\n",
    "\n",
    "# Models\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from xgboost import XGBRegressor, XGBClassifier, plot_importance, plot_tree\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix, classification_report, make_scorer\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_recall_curve\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Ignore useless warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "# Preset data display\n",
    "pd.options.display.max_seq_items = 5000\n",
    "pd.options.display.max_rows = 5000\n",
    "\n",
    "# Set palette\n",
    "flatui = [\"#9b59b6\", \"#3498db\", \"#95a5a6\", \"#e74c3c\", \"#34495e\", \"#2ecc71\"]\n",
    "sns.set_palette(flatui)\n",
    "sns.palplot(sns.color_palette(flatui))\n",
    "#34495e\n",
    "\n",
    "seed = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pct_bar_labels():\n",
    "    '''\n",
    "    Function used to label the relative frequency on top of each bars\n",
    "    '''\n",
    "    # Set font size\n",
    "    fs=15\n",
    "    \n",
    "    # Set plot label and ticks\n",
    "    plt.ylabel('Relative Frequency (%)', fontsize=fs)\n",
    "    plt.xticks(rotation=0, fontsize=fs)\n",
    "    plt.yticks([])\n",
    "    \n",
    "    # Set individual bar labels in proportional scale\n",
    "    for x in ax1.patches:\n",
    "        ax1.annotate(str(x.get_height()) + '%', \n",
    "        (x.get_x() + x.get_width()/2., x.get_height()), ha='center', va='center', xytext=(0, 7), \n",
    "        textcoords='offset points', fontsize=fs, color='black')\n",
    "\n",
    "def freq_table(var):\n",
    "    '''\n",
    "    Define plot global variables\n",
    "    Create a function that will populate a frequency table (%)\n",
    "    Get counts per feature then get the percentage over the total counts\n",
    "    '''\n",
    "    global ax, ax1\n",
    "    \n",
    "    # Get Values and pct and combine it into a dataframe\n",
    "    count_freq = var.value_counts()\n",
    "    pct_freq = round(var.value_counts(normalize=True)*100, 2)\n",
    "    \n",
    "    # Create a dataframe\n",
    "    df = pd.DataFrame({'Count': count_freq, 'Percentage': pct_freq})\n",
    "    \n",
    "    # Print variable name\n",
    "    print('Frequency of', var.name, ':')\n",
    "    display(df)\n",
    "    \n",
    "    # Create plot\n",
    "    ax1 = pct_freq.plot.bar(title='Percentage of {}'.format(var.name), figsize=(12,8))\n",
    "    ax1.title.set_size(15)\n",
    "    pct_bar_labels()\n",
    "    plt.show()\n",
    "    \n",
    "# Define a null function\n",
    "def get_nulls(df):\n",
    "    \n",
    "    # Get null pct and counts\n",
    "    null_cols = pd.DataFrame(df.isnull().sum().sort_values(ascending=False), columns=['Null Data Count'])\n",
    "    null_cols_pct = pd.DataFrame(round(df.isnull().sum().sort_values(ascending=False)/len(df),5), columns=['Null Data Pct'])\n",
    "\n",
    "    # Combine dataframes horizontally\n",
    "    null_cols_df = pd.DataFrame(pd.concat([null_cols, null_cols_pct], axis=1))\n",
    "\n",
    "    all_nulls = null_cols_df[null_cols_df['Null Data Pct']>0]\n",
    "\n",
    "    # Print\n",
    "    print('There are', len(all_nulls), 'columns with missing values.')\n",
    "    return all_nulls\n",
    "\n",
    "# Define plot_nulls function\n",
    "def plot_nulls(train):\n",
    "    # Get null pct and counts\n",
    "    null_cols = pd.DataFrame(train.isnull().sum().sort_values(ascending=False), columns=['Null Data Count'])\n",
    "    null_cols_pct = pd.DataFrame(round(train.isnull().sum().sort_values(ascending=False)/len(train),5)*100, columns=['Null Data %'])\n",
    "\n",
    "    # Combine horizontally (axis=1) into a dataframe with column names (keys=[]) then to a data frame\n",
    "    null_cols_df = pd.DataFrame(pd.concat([null_cols, null_cols_pct], axis=1))\n",
    "\n",
    "    all_nulls = null_cols_df[null_cols_df['Null Data %']>0]\n",
    "\n",
    "    # Create figure space\n",
    "    if len(all_nulls) > 8:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "    elif len(all_nulls) > 5:\n",
    "        plt.figure(figsize=(6, 8))\n",
    "    else:\n",
    "        plt.figure(figsize=(4, 8))\n",
    "\n",
    "    # Create plot\n",
    "    sns.barplot(x=all_nulls.index,\n",
    "                y='Null Data %',\n",
    "                data=all_nulls)\n",
    "\n",
    "    # Set plot features\n",
    "    plt.ylim(0, 100)\n",
    "    plt.xticks(rotation='90')\n",
    "    plt.xlabel('Features', fontsize=15)\n",
    "    plt.ylabel('Percent of Missing Values', fontsize=15)\n",
    "    plt.title('Percent of Missing Data by Features', fontsize=15)\n",
    "    plt.show()\n",
    "    \n",
    "# Create a new function to capture feature importance for models\n",
    "def feature_importance(model):\n",
    "    \n",
    "    importance = pd.DataFrame({'Feature': headers,\n",
    "                               'Importance': np.round(model.feature_importances_,3)})\n",
    "    \n",
    "    importance = importance.sort_values(by = 'Importance', ascending = False).set_index('Feature')\n",
    "    \n",
    "    return importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Apply PCA by fitting the good data with only two dimensions\n",
    "pca = PCA(n_components=2).fit(good_data)\n",
    "\n",
    "# TODO: Transform the good data using the PCA fit above\n",
    "reduced_data = pca.transform(good_data)\n",
    "\n",
    "# TODO: Transform log_samples using the PCA fit above\n",
    "pca_samples = pca.transform(log_samples)\n",
    "\n",
    "# Create a DataFrame for the reduced data\n",
    "reduced_data = pd.DataFrame(reduced_data, columns = ['Dimension 1', 'Dimension 2'])\n",
    "\n",
    "# Display sample log-data after applying PCA transformation in two dimensions\n",
    "display(pd.DataFrame(np.round(pca_samples, 4), columns = ['Dimension 1', 'Dimension 2']))\n",
    "\n",
    "# Create a biplot\n",
    "vs.biplot(good_data, reduced_data, pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
