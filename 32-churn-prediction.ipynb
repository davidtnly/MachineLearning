{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Classification: Predicting Employee Churn (2019-10-31)\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition\n",
    "--------\n",
    "### Project Overview\n",
    "As a member of the HR/People Analytics department, my goal in the team is to provide a data-driven approach on how the company can manage people at work. Problems that are addressed within my team could consist of:\n",
    "- Hiring & Assessment\n",
    "- Retention\n",
    "- Performance\n",
    "- Learning & Development\n",
    "- Team Composition\n",
    "\n",
    "This notebook is geared towards reviewing the work I've been doing (there will be a lot of text) and practice implementing a fully structured analysis on a business problem: reducing employee churn. Evaluate the data to find out why employees are leaving the company, and develop a model to predict who will leave the company so the company can be more proactive in reducing churn.\n",
    "\n",
    "\"... study by the Society for Human Resource Management states that the average cost to hire an employee is 4,129, with around 42 days to fill a position. According to Glassdoor, the average company in the United States spends about 4,000 to hire a new employee, taking up to 52 days to fill a position.\"\n",
    "\n",
    "### What exactly is employee churn?\n",
    "Employee churn is when employees depart from a company or organization. Whether it would be voluntary or involuntary, termination is the result of a churn. Employee churn can be affected by age, tenure, salary, job satisfaction, working conditions, growth potential, environment, team, and much more. \n",
    "\n",
    "Like all new implementations and onboarding,  acquiring new employees as a replacement has its costs such as hiring and training. Also, the new employee will need time to adapt to the new environment and learn skills required for the position since all jobs are different in their own specific way whether it's the business objective to a specific way technology is set up for the team. Organizations can tackle this problem by applying machine learning techniques to predict employee churn, which helps them in taking necessary actions.\n",
    "\n",
    "### Problem Statement\n",
    "As a company, are we able to figure out why employees are leaving the company and develop solutions to prevent it from happening? The cost and time it takes to hire a \"good\" employee is inefficient and can be expensive. We need to be diligent to reduce attrition rates and increase retention rates as much as possible.\n",
    "\n",
    "### Results\n",
    "TODO\n",
    "\n",
    "### Methods to Reduce Employee Churn\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "-----------\n",
    "### Characteristics\n",
    "\n",
    "#### Dimensions\n",
    "There are a total of 14,999 rows of data and 10 columns (features).\n",
    "\n",
    "#### Features\n",
    "\n",
    "- satisfaction_level (0â€“1)\n",
    "- last_evaluation (time since last evaluation in years)\n",
    "- number_projects (number of projects completed while at work)\n",
    "- average_monthly_hours (average monthly hours at workplace)\n",
    "- time_spend_company (time spent at the company in years)\n",
    "- Work_accident (whether the employee had a workplace accident)\n",
    "- left (whether the employee left the workplace or not (1 or 0))\n",
    "- promotion_last_5 years (whether the employee was promoted in the last five years)\n",
    "- sales (department in which they work for)\n",
    "- salary (relative level of salary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation & Validation Method\n",
    "----------\n",
    "This is a classification problem to predict the outcome of churn or no churn, so we will focus on algorithms like logistic regression, random forest, gradient boosting, regularized linear algorithms (lasso, ridge, elastic-net), and support vector machine. We can also create an ensemble model that will consist of stacking or blending, which utilizes meta-learners.\n",
    "\n",
    "After each performance of the model, I will evaluate the benchmark model and then cross-validate it to make sure the model performance is up to standard on unseen data using the validation method K-fold. \n",
    "\n",
    "This is a popular cv method where the training dataset is split into k number of partitions (folds). If the dataset has 100 observations and k =10 then there would be 10 folds each having 10 observations. Out of the 10 folds, 1 of them will be considered a test set and the rest for retraining purposes. Once that first split has been trained and tested, it will go on to the next random split of training, testing, and evaluation. After the end of the 10 splits, the results are averaged together to get a better idea of our model performance.\n",
    "\n",
    "#### Evaluation Metric\n",
    "- Accuracy: (TP + TN) / (TP + FP + TN + FN)\n",
    "\n",
    "Here are some performance metrics used in the confusion matrix below:\n",
    "\n",
    "- TP - correct positive prediction\n",
    "- TN - correct negative prediction\n",
    "- FP - Type I Error (case negative but predicted positive)\n",
    "- FN - Type II Error (case positive but predicted negative)\n",
    "- Precision - how accurate is the model of those predicted positive that are actually positive\n",
    "    - TP/(TP + FP)\n",
    "- Recall / Sensitivity / TPR (True Positive Rate) - how many actual positives the model labeled over the total positives\n",
    "    - TP/(TP + FN) which is also TP/All P\n",
    "- Specificity (True Negative Rate) - number of correct negative predictions\n",
    "    - TN/(TN + FP) which is also TN/All N\n",
    "- F1 Score - weighted average of TPR (Recall) and Precision, useful if there is uneven class distribution like cancer detection where there can be 10,000 negatives and only 1 positive or if there is a higher risk if false negatives or false positives\n",
    "    - 2 * ( (Pr * Rc) / (Pr + Rc) )\n",
    "- ROC Curve\n",
    "    - Graph used to summarize the performance at various thresholds by plotting the TPR against FPR\n",
    "    \n",
    "### Benchmark Model\n",
    "\n",
    "We will use a simple logistic model to use as our benchmark model. We can review complexity, speed, memory usage, accuracy, and interpretibility as we seek the best model that can be used for the company.\n",
    "\n",
    "TODO\n",
    "\n",
    "### Best Model\n",
    "TODO\n",
    "\n",
    "### Methods to Improve Model Performance\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Methodology\n",
    "----------\n",
    "I like to add this framework here so I can always go back to it and check if I need to work on a certain step or want to add more to it. I don't necessarily need to do every step.\n",
    "\n",
    "1. Framing the problem\n",
    "    - What are we trying to solve?\n",
    "    - Understand what's the problem here and ask questions\n",
    "    - What type of problem are we trying to solve?\n",
    "        - Classification, regression, etc.\n",
    "        - Types of algorithms specified for possible use\n",
    "2. Collecting relevant information and data\n",
    "    - What type of data do we have?\n",
    "    - Can we use any complimentary data with that is public that will help our with our analysis/modeling?\n",
    "    - What other data requirements are there?\n",
    "    - What is considered a success for this problem?\n",
    "    - What libraries do we need?\n",
    "        - How do we know if our models are good?\n",
    "3. Process for analysis (preprocessing & cleaning)\n",
    "    - How does the data structure and distribution look like?\n",
    "    - Is the data usable right away?\n",
    "    - Can the data be plotted?\n",
    "    - What changes do we need in order to make the data usable if it's not already?\n",
    "    - This step is not really an isolated step as it can encompass exploratory, feature engineering + more\n",
    "4. Exploratory data analysis\n",
    "    - How does the data look like?\n",
    "    - Are there any patterns?\n",
    "        - Identify any summary statistics, plotting, counting, etc.\n",
    "    - Familiarize yourself with the data\n",
    "    - Basically the step to help you get to know the data better\n",
    "5. Feature engineering (applied machine learning)\n",
    "    - Can we create more data (features) that will be helpful for our models?\n",
    "    - \"... re-working of predictors is more of an art, requiring the right tools and experience to find better predictor representations\" - Max Kuhn\n",
    "6. Statistical analysis\n",
    "    - Univariate, bivariate, multivariate analysis\n",
    "        - Analysis of a single feature\n",
    "        - Analysis of two features and their relationships\n",
    "        - Analysis of data collected on more than one dependent variable and their relationships (PCA, PLS)\n",
    "7. Model development & scoring\n",
    "    - Splitting the data into train and test sets\n",
    "        - Always make sure you have a completely separate data to test your final model on after hyperparameter tuning and training has been done\n",
    "        - Think about your experimental design beforehand so that you minimize unrelated sources of variation and reduce as much data leakage (if any) as possible\n",
    "    - Normalizing the data to be on similar scales\n",
    "        - Normalization, standardization methods (Z-score, minmax, median)\n",
    "    - Create baseline, pre-tuned, and tuned models (includes cross-validation)\n",
    "        - Create easy to more complex models if needed (think about computational cost, complexity, explainability)\n",
    "    - Hyperparameter tuning\n",
    "    - Fit cross-validated tuned models using best hyperparameters\n",
    "    - Score model and get results\n",
    "8. Evaluation\n",
    "    - How accurate are the models?\n",
    "        - Are the models overfitting or underfitting?\n",
    "    - What evaluation metric are we using?\n",
    "    - Is the final model good enough?\n",
    "    - Which features are important?\n",
    "    - Iterate steps if we are proceeding with specific features selected\n",
    "        - Dimensionality reduction methods (PCA, LDA) to see if we can reduce model complexity (this step can be before modeling as well)\n",
    "9. Results\n",
    "    - What's our conclusion?\n",
    "    - What actions are we going to take?\n",
    "10. Ending notes\n",
    "    - Extra information that we may not go over like steps that we did not go into or missed\n",
    "        - Any other possible methods/solutions that we could look at in the future\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAABICAYAAADI6S+jAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAB90lEQVR4nO3aMWpUURTH4fPERbgAuVNaSEqxVsgCTGMjSKaJuIGMG7AUBJs0cQGCtoJlsEh5wQW4i2tjOTgEfPln3nwf3ObxinOa3wyXN40xCoDbdy89AMChEmCAEAEGCBFggBABBggRYICQ+7teWK1Wm6o6r6pan67r7M3Z3DMBLM209eENvwMen06+/Z9x7qBXl8/q6OOv9BizuHr9sD5cfk6PMZv1yYv6ffwkPcZsHnz5UUfPX6bHmM3V14t69PNteozZXD9+vzXAriAAQgQYIESAAUIEGCBEgAFCBBggRIABQgQYIESAAUIEGCBEgAFCBBggRIABQgQYIESAAUIEGCBEgAFCBBggRIABQgQYIESAAUIEGCBEgAFCBBggRIABQgQYIESAAUIEGCBEgAFCBBggRIABQgQYIESAAUIEGCBEgAFCBBggRIABQgQYIESAAUIEGCBEgAFCBBggRIABQgQYIESAAUIEGCBEgAFCBBggRIABQgQYIESAAUIEGCBljPHP01rbtNbG3/N91/v7fFprm/QMdrOf/Q7n7PwH3Hvf9N6n3vtUVU9v4Tch6Tw9wIyWvFuV/fbd0vfbyhUEQIgAA4TcNMDvZpni7ljyfkvercp++27p+201jTHSMwAcJFcQACECDBAiwAAhAgwQIsAAIX8Ag3g5X4JsnGAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Toolbox 101\n",
    "from datetime import datetime, timedelta, date\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import gc\n",
    "import os\n",
    "import pandas as pd\n",
    "import pandas_profiling\n",
    "import numpy as np\n",
    "import random as rand\n",
    "import datetime as dt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# import plotly.plotly as py\n",
    "import plotly.graph_objs as go # graph_objects in version 4 (currently have plotly v3 installed)\n",
    "import plotly.offline as pyoff\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "# Stats\n",
    "from scipy.stats import norm, skew\n",
    "from scipy.special import boxcox1p\n",
    "from scipy.stats import boxcox_normmax\n",
    "from scipy import stats\n",
    "import statsmodels.formula.api as smf \n",
    "from statsmodels.stats.anova import anova_lm\n",
    "from statsmodels.stats import power\n",
    "\n",
    "# Models\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBRegressor, XGBClassifier, plot_importance, plot_tree\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix, classification_report, make_scorer\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_recall_curve\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import confusion_matrix, classification_report, make_scorer\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_recall_curve, roc_auc_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Ignore useless warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "# Preset data display\n",
    "pd.options.display.max_seq_items = 5000\n",
    "pd.options.display.max_rows = 5000\n",
    "\n",
    "# Set palette\n",
    "flatui = [\"#9b59b6\", \"#3498db\", \"#95a5a6\", \"#e74c3c\", \"#34495e\", \"#2ecc71\"]\n",
    "sns.set_palette(flatui)\n",
    "sns.palplot(sns.color_palette(flatui))\n",
    "# 34495e\n",
    "\n",
    "seed = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pct_bar_labels():\n",
    "    '''\n",
    "    Function used to label the relative frequency on top of each bars\n",
    "    '''\n",
    "    # Set font size\n",
    "    fs=15\n",
    "    \n",
    "    # Set plot label and ticks\n",
    "    plt.ylabel('Relative Frequency (%)', fontsize=fs)\n",
    "    plt.xticks(rotation=0, fontsize=fs)\n",
    "    plt.yticks([])\n",
    "    \n",
    "    # Set individual bar labels in proportional scale\n",
    "    for x in ax1.patches:\n",
    "        ax1.annotate(str(x.get_height()) + '%', \n",
    "        (x.get_x() + x.get_width()/2., x.get_height()), ha='center', va='center', xytext=(0, 7), \n",
    "        textcoords='offset points', fontsize=fs, color='black')\n",
    "\n",
    "def freq_table(var):\n",
    "    '''\n",
    "    Define plot global variables\n",
    "    Create a function that will populate a frequency table (%)\n",
    "    Get counts per feature then get the percentage over the total counts\n",
    "    '''\n",
    "    global ax, ax1\n",
    "    \n",
    "    # Get Values and pct and combine it into a dataframe\n",
    "    count_freq = var.value_counts()\n",
    "    pct_freq = round(var.value_counts(normalize=True)*100, 2)\n",
    "    \n",
    "    # Create a dataframe\n",
    "    df = pd.DataFrame({'Count': count_freq, 'Percentage': pct_freq})\n",
    "    \n",
    "    # Print variable name\n",
    "    print('Frequency of', var.name, ':')\n",
    "    display(df)\n",
    "    \n",
    "    # Create plot\n",
    "    ax1 = pct_freq.plot.bar(title='Percentage of {}'.format(var.name), figsize=(12,8))\n",
    "    ax1.title.set_size(15)\n",
    "    pct_bar_labels()\n",
    "    plt.show()\n",
    "    \n",
    "# Define a null function\n",
    "def get_nulls(df):\n",
    "    \n",
    "    # Get null pct and counts\n",
    "    null_cols = pd.DataFrame(df.isnull().sum().sort_values(ascending=False), columns=['Null Data Count'])\n",
    "    null_cols_pct = pd.DataFrame(round(df.isnull().sum().sort_values(ascending=False)/len(df),2), columns=['Null Data Pct'])\n",
    "\n",
    "    # Combine dataframes horizontally\n",
    "    null_cols_df = pd.DataFrame(pd.concat([null_cols, null_cols_pct], axis=1))\n",
    "\n",
    "    all_nulls = null_cols_df[null_cols_df['Null Data Pct']>0]\n",
    "\n",
    "    # Print\n",
    "    print('There are', len(all_nulls), 'columns with missing values.')\n",
    "    return all_nulls\n",
    "\n",
    "# Define plot_nulls function\n",
    "def plot_nulls(train):\n",
    "    # Get null pct and counts\n",
    "    null_cols = pd.DataFrame(train.isnull().sum().sort_values(ascending=False), columns=['Null Data Count'])\n",
    "    null_cols_pct = pd.DataFrame(round(train.isnull().sum().sort_values(ascending=False)/len(train),2)*100, columns=['Null Data %'])\n",
    "\n",
    "    # Combine horizontally (axis=1) into a dataframe with column names (keys=[]) then to a data frame\n",
    "    null_cols_df = pd.DataFrame(pd.concat([null_cols, null_cols_pct], axis=1))\n",
    "\n",
    "    all_nulls = null_cols_df[null_cols_df['Null Data %']>0]\n",
    "\n",
    "    # Create figure space\n",
    "    if len(all_nulls) > 8:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "    elif len(all_nulls) > 5:\n",
    "        plt.figure(figsize=(6, 8))\n",
    "    else:\n",
    "        plt.figure(figsize=(4, 8))\n",
    "\n",
    "    # Create plot\n",
    "    sns.barplot(x=all_nulls.index,\n",
    "                y='Null Data %',\n",
    "                data=all_nulls)\n",
    "\n",
    "    # Set plot features\n",
    "    plt.ylim(0, 100)\n",
    "    plt.xticks(rotation='90')\n",
    "    plt.xlabel('Features', fontsize=15)\n",
    "    plt.ylabel('Percent of Missing Values', fontsize=15)\n",
    "    plt.title('Percent of Missing Data by Features', fontsize=15)\n",
    "    plt.show()\n",
    "    \n",
    "# Create a new function to capture feature importance for models\n",
    "def feature_importance(model):\n",
    "    \n",
    "    importance = pd.DataFrame({'Feature': headers,\n",
    "                               'Importance': np.round(model.feature_importances_,3)})\n",
    "    \n",
    "    importance = importance.sort_values(by = 'Importance', ascending = False).set_index('Feature')\n",
    "    \n",
    "    return importance\n",
    "\n",
    "# Reduce data size\n",
    "def reduce_mem_usage(df):\n",
    "    ''' \n",
    "    Iterate through all the columns of a dataframe and modify the data type\n",
    "    to reduce memory usage.        \n",
    "    '''\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    # Loop through every column in the dataframe\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        # Objects\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            \n",
    "            # Numbers\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['01-ign.csv',\n",
       " '02-winequality-red.csv',\n",
       " '02-winequality-white.csv',\n",
       " '03-thanksgiving-2015-poll-data.csv',\n",
       " '05-ibm-sales-loss.csv',\n",
       " '07-test.csv',\n",
       " '07-train.csv',\n",
       " '09-house-regression-env.db',\n",
       " '09-house-test.csv',\n",
       " '09-house-train.csv',\n",
       " '10-vgsales.csv',\n",
       " '11-diabetes.csv',\n",
       " '13-facebook-likes.csv',\n",
       " '15-google-review-ratings.csv',\n",
       " '16-mnist-test.csv',\n",
       " '16-mnist-train.csv',\n",
       " '17-real-estate-valuation.csv',\n",
       " '18-credit-card-defaults.csv',\n",
       " '20-zoo.csv',\n",
       " '21-breast-cancer.csv',\n",
       " '26-telco.csv',\n",
       " '32-churn.csv',\n",
       " 'README.md']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/32-churn.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train transaction dataset has 14999 rows and 10 columns.\n"
     ]
    }
   ],
   "source": [
    "print(f'Train transaction dataset has {data.shape[0]} rows and {data.shape[1]} columns.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>satisfaction_level</th>\n",
       "      <th>last_evaluation</th>\n",
       "      <th>number_project</th>\n",
       "      <th>average_montly_hours</th>\n",
       "      <th>time_spend_company</th>\n",
       "      <th>Work_accident</th>\n",
       "      <th>left</th>\n",
       "      <th>promotion_last_5years</th>\n",
       "      <th>Departments</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.53</td>\n",
       "      <td>2</td>\n",
       "      <td>157</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5</td>\n",
       "      <td>262</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.11</td>\n",
       "      <td>0.88</td>\n",
       "      <td>7</td>\n",
       "      <td>272</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.87</td>\n",
       "      <td>5</td>\n",
       "      <td>223</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.37</td>\n",
       "      <td>0.52</td>\n",
       "      <td>2</td>\n",
       "      <td>159</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   satisfaction_level  last_evaluation  number_project  average_montly_hours  \\\n",
       "0                0.38             0.53               2                   157   \n",
       "1                0.80             0.86               5                   262   \n",
       "2                0.11             0.88               7                   272   \n",
       "3                0.72             0.87               5                   223   \n",
       "4                0.37             0.52               2                   159   \n",
       "\n",
       "   time_spend_company  Work_accident  left  promotion_last_5years  \\\n",
       "0                   3              0     1                      0   \n",
       "1                   6              0     1                      0   \n",
       "2                   4              0     1                      0   \n",
       "3                   5              0     1                      0   \n",
       "4                   3              0     1                      0   \n",
       "\n",
       "  Departments   salary  \n",
       "0        sales     low  \n",
       "1        sales  medium  \n",
       "2        sales  medium  \n",
       "3        sales     low  \n",
       "4        sales     low  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>satisfaction_level</th>\n",
       "      <th>last_evaluation</th>\n",
       "      <th>number_project</th>\n",
       "      <th>average_montly_hours</th>\n",
       "      <th>time_spend_company</th>\n",
       "      <th>Work_accident</th>\n",
       "      <th>left</th>\n",
       "      <th>promotion_last_5years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>14999.000000</td>\n",
       "      <td>14999.000000</td>\n",
       "      <td>14999.000000</td>\n",
       "      <td>14999.000000</td>\n",
       "      <td>14999.000000</td>\n",
       "      <td>14999.000000</td>\n",
       "      <td>14999.000000</td>\n",
       "      <td>14999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.612834</td>\n",
       "      <td>0.716102</td>\n",
       "      <td>3.803054</td>\n",
       "      <td>201.050337</td>\n",
       "      <td>3.498233</td>\n",
       "      <td>0.144610</td>\n",
       "      <td>0.238083</td>\n",
       "      <td>0.021268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.248631</td>\n",
       "      <td>0.171169</td>\n",
       "      <td>1.232592</td>\n",
       "      <td>49.943099</td>\n",
       "      <td>1.460136</td>\n",
       "      <td>0.351719</td>\n",
       "      <td>0.425924</td>\n",
       "      <td>0.144281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.440000</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>156.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.820000</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>245.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       satisfaction_level  last_evaluation  number_project  \\\n",
       "count        14999.000000     14999.000000    14999.000000   \n",
       "mean             0.612834         0.716102        3.803054   \n",
       "std              0.248631         0.171169        1.232592   \n",
       "min              0.090000         0.360000        2.000000   \n",
       "25%              0.440000         0.560000        3.000000   \n",
       "50%              0.640000         0.720000        4.000000   \n",
       "75%              0.820000         0.870000        5.000000   \n",
       "max              1.000000         1.000000        7.000000   \n",
       "\n",
       "       average_montly_hours  time_spend_company  Work_accident          left  \\\n",
       "count          14999.000000        14999.000000   14999.000000  14999.000000   \n",
       "mean             201.050337            3.498233       0.144610      0.238083   \n",
       "std               49.943099            1.460136       0.351719      0.425924   \n",
       "min               96.000000            2.000000       0.000000      0.000000   \n",
       "25%              156.000000            3.000000       0.000000      0.000000   \n",
       "50%              200.000000            3.000000       0.000000      0.000000   \n",
       "75%              245.000000            4.000000       0.000000      0.000000   \n",
       "max              310.000000           10.000000       1.000000      1.000000   \n",
       "\n",
       "       promotion_last_5years  \n",
       "count           14999.000000  \n",
       "mean                0.021268  \n",
       "std                 0.144281  \n",
       "min                 0.000000  \n",
       "25%                 0.000000  \n",
       "50%                 0.000000  \n",
       "75%                 0.000000  \n",
       "max                 1.000000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14999 entries, 0 to 14998\n",
      "Data columns (total 10 columns):\n",
      "satisfaction_level       14999 non-null float64\n",
      "last_evaluation          14999 non-null float64\n",
      "number_project           14999 non-null int64\n",
      "average_montly_hours     14999 non-null int64\n",
      "time_spend_company       14999 non-null int64\n",
      "Work_accident            14999 non-null int64\n",
      "left                     14999 non-null int64\n",
      "promotion_last_5years    14999 non-null int64\n",
      "Departments              14999 non-null object\n",
      "salary                   14999 non-null object\n",
      "dtypes: float64(2), int64(6), object(2)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing / Cleaning\n",
    "Here, I'll will take a look at the data more in depth. This will give me an idea on how the data looks and it is important to know the data at a more granular level versus summary level. Although there are only 10 features, I can't be too overly confident that I will be an expert on it right away. Be sure to look at the structure (done above), the distributions, correlations, etc.\n",
    "\n",
    "I want to make sure there are no missing values if possible, and if it's missing understanding why it's missing. Missing values can mess up the data especially in regression modeling. List out some methods on ways to tackle the missing value problem.\n",
    "\n",
    "#### Structure\n",
    "I will create plots and tables depending on which will be more effective. Not only that, it can also depend on how I want the data to be viewed in the notebook especially if I know my audience. Visuals are good if it's to a high level audience and can be broken down if I am looking to deep dive into that particular section/feature.\n",
    "\n",
    "In summary, some of the questions I can try to answer in this section are:\n",
    "1. Are we able to use the data right away?\n",
    "    - If not, why?\n",
    "    \n",
    "    \n",
    "2. Are there any missing values?\n",
    "    - What are some methods to combat missing values?\n",
    "    \n",
    "    \n",
    "3. How clean is the data?\n",
    "    - Are all value types consistent and correct?\n",
    "    - Do I need to remove any features because of missing or corrupt values?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce memory\n",
    "Using the function, reduce the feature sizes of the data which will help speed up the entire process. It is also good to be memory efficient especially with datasets ranging in the millions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 1.14 MB\n",
      "Memory usage after optimization is: 0.19 MB\n",
      "Decreased by 83.7%\n"
     ]
    }
   ],
   "source": [
    "print('-' * 80)\n",
    "print('Data')\n",
    "data = reduce_mem_usage(data)\n",
    "print('-' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "satisfaction_level        float16\n",
       "last_evaluation           float16\n",
       "number_project               int8\n",
       "average_montly_hours        int16\n",
       "time_spend_company           int8\n",
       "Work_accident                int8\n",
       "left                         int8\n",
       "promotion_last_5years        int8\n",
       "Departments              category\n",
       "salary                   category\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the data types sizes were all 64, which is now reduced to either 16 or 8. This reduced the dataset from approximately 1.14MB to 0.19MB; a the memory usage drop by almost 84%. The one thing that I need to be aware of is loss of information. The steps taken to reduce the dataset uses the following approach:\n",
    "1. Iterate through every column\n",
    "2. Determine if the column is numeric\n",
    "3. Determine if the column can be represented by an integer\n",
    "4. Find the min and max value\n",
    "5. Determine and apply the smallest datatype that can fit the range of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus\n",
    "------\n",
    "### Data storytelling \n",
    "Data storytelling can be a whole notebook by itself on best practices on what can be done to craft a more compelling story behind my analyses to uncover insights and drive action. Remember that all the work I'm doing is really just a subset of a giant interdisciplinary field and for me, this subset involves behavioral economics (we are all irrational), psychology (behavioral, neuroscience), decision science, and much more!\n",
    "\n",
    "Take a look at some of these links about data storytelling/data visualization:\n",
    "- [Storytellingwithdata](http://www.storytellingwithdata.com/)\n",
    "- [Data Storytelling as an essential skill](https://www.forbes.com/sites/brentdykes/2016/03/31/data-storytelling-the-essential-data-science-skill-everyone-needs/#6e2ba8e652ad)\n",
    "- [Few examples](https://twooctobers.com/blog/8-data-storytelling-concepts-with-examples/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (TensorFlow)",
   "language": "python",
   "name": "tf-test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
