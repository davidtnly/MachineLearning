{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid function suffers from the problem of “vanishing gradients” as it flattens out at both ends, resulting in very small changes in the weights during backpropagation. This can make the neural network refuse to learn and get stuck. Due to this reason, usage of the sigmoid function is being replaced by other non-linear functions such as Rectified Linear Unit (ReLU)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An __activation function__ is a function which is applied to the output of a neural network layer, which is then passed as the input to the next layer. Activation functions are an essential part of neural networks as they provide non-linearity, without which the neural network reduces to a mere logistic regression model. The most widely used activation function is the __Rectified Linear Unit (ReLU)__.\n",
    "\n",
    "- ReLU is defined as f(x) = max(0, x).\n",
    "\n",
    "\n",
    "- Computationally faster: The ReLU is a highly simplified function which is easily computed.\n",
    "\n",
    "\n",
    "- Fewer vanishing gradients: In machine learning, the update to a parameter is proportional to the partial derivative of the error function with respect to that parameters. If the gradient becomes extremely small, the updates will not be effective and the network might stop training at all. The ReLU does not saturate in the positive direction, whereas other activation functions like sigmoid and hyperbolic tangent saturate in both directions. Therefore, it has fewer vanishing gradients resulting in better training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input type: Tensor(\"Const:0\", shape=(6,), dtype=float32)\n",
      "Input: [ 1.  -0.5  3.4 -2.1  0.  -6.5]\n",
      "Return type: Tensor(\"ReLU:0\", shape=(6,), dtype=float32)\n",
      "Output: [1.  0.  3.4 0.  0.  0. ]\n"
     ]
    }
   ],
   "source": [
    "# Importing the Tensorflow library \n",
    "import tensorflow as tf \n",
    "  \n",
    "# A constant vector of size 6 \n",
    "a = tf.constant([1.0, -0.5, 3.4, -2.1, 0.0, -6.5], dtype=tf.float32) \n",
    "  \n",
    "# Applying the ReLu function and \n",
    "# storing the result in 'b' \n",
    "b = tf.nn.relu(a, name='ReLU') \n",
    "  \n",
    "# Initiating a Tensorflow session \n",
    "with tf.Session() as sess: \n",
    "    print('Input type:', a) \n",
    "    print('Input:', sess.run(a)) \n",
    "    print('Return type:', b) \n",
    "    print('Output:', sess.run(b)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leaky ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ReLU function suffers from what is called the “__dying ReLU__” problem. Since the slope of the ReLU function on the negative side is zero, a neuron stuck on that side is unlikely to recover from it. This causes the neuron to output zero for every input, thus rendering it useless. A solution to this problem is to use Leaky ReLU which has a small slope on the negative side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input type: Tensor(\"Const_3:0\", shape=(6,), dtype=float32)\n",
      "Input: [ 1.  -0.5  3.4 -2.1  0.  -6.5]\n",
      "Return type: Tensor(\"Leaky_ReLU_2:0\", shape=(6,), dtype=float32)\n",
      "Output: [ 1.    -0.005  3.4   -0.021  0.    -0.065]\n"
     ]
    }
   ],
   "source": [
    "# Importing the Tensorflow library \n",
    "import tensorflow as tf  \n",
    "  \n",
    "# A constant vector of size 6 \n",
    "a = tf.constant([1.0, -0.5, 3.4, -2.1, 0.0, -6.5], dtype=tf.float32) \n",
    "  \n",
    "# Applying the Leaky ReLu function with \n",
    "# slope 0.01 and storing the result in 'b' \n",
    "b = tf.nn.leaky_relu(a, alpha=0.01, name='Leaky_ReLU') \n",
    "  \n",
    "# Initiating a Tensorflow session \n",
    "with tf.Session() as sess:  \n",
    "    print('Input type:', a) \n",
    "    print('Input:', sess.run(a)) \n",
    "    print('Return type:', b) \n",
    "    print('Output:', sess.run(b)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (TensorFlow)",
   "language": "python",
   "name": "tf-test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
